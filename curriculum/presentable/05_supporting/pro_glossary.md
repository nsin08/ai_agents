# Pro Glossary

**Level**: Pro
**Scope**: Research and innovation concepts

---

## Terms

1. **Benchmark suite**: A curated set of tests for model evaluation.
2. **Rater agreement**: Consistency between human evaluators.
3. **A/B test**: Controlled comparison between two variants.
4. **Counterfactual**: Alternate scenario used to test robustness.
5. **Red teaming**: Structured adversarial testing for safety.
6. **Adversarial prompt**: Input designed to bypass guardrails.
7. **Calibration**: Alignment between confidence and correctness.
8. **Pareto frontier**: Trade-off curve between competing objectives.
9. **Cost curve**: Relationship between cost and quality.
10. **Systemic bias**: Bias in outputs due to data or model design.
11. **Preference modeling**: Learning user preferences over time.
12. **Reward model**: Model used to score outputs in RLHF.
13. **Evaluation harness**: Tooling that runs standardized evaluations.
14. **Data provenance**: Traceability of data sources.
15. **Reproducibility**: Ability to repeat results with same setup.
16. **Drift detection**: Monitoring for performance changes over time.

---

## Document Checklist

- [ ] Accessibility review (WCAG AA)
- [ ] At least 15 terms included
- [ ] Definitions are concise
- [ ] ASCII only

