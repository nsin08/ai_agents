# Local Development Configuration
# Optimized for local development with Ollama

app:
  name: ai_agent_local
  mode: development
  debug: true
  log_level: DEBUG

models:
  provider: ollama
  model: llama2
  base_url: http://localhost:11434
  timeout: 60
  temperature: 0.7
  # max_tokens: 2048  # Optional

tools:
  timeout: 30
  temperature: 0.3
  max_length: 500
  # allowlist:  # Optional - restrict tools
  #   - calculator
  #   - web_search

memory:
  short_term_size: 10
  long_term_enabled: false
  context_window: 4096

engine:
  max_turns: 10
  timeout: 300
  enable_reflection: true

observability:
  enable_tracing: true
  enable_metrics: false
  log_prompts: true
  log_responses: true
