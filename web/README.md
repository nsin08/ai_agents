# Web-Based Agent Chat Interface ‚Äî Learning Lab

**Purpose**: Hands-on demonstration of a production-ready AI agent chat interface with multi-provider support, debug observability, and configuration management. This lab serves as a practical reference for building web-based agent applications that integrate with the broader AI agents learning curriculum.

---

## üéØ Learning Objectives

This lab teaches you how to:

1. **Build a multi-provider agent interface** ‚Äî Support 6 LLM providers (Mock, Ollama, OpenAI, Anthropic, Google, Azure) with dynamic switching
2. **Implement debug observability** ‚Äî Display 14 performance metrics (tokens, latency, reasoning steps, cost estimation)
3. **Design configuration management** ‚Äî Create presets (Creative/Precise/Balanced) with sliders and validation
4. **Export conversations** ‚Äî Generate JSON and Markdown exports with metadata
5. **Implement accessibility** ‚Äî ARIA labels, keyboard navigation, semantic HTML (WCAG AA compliant)
6. **Test comprehensively** ‚Äî 61 tests (44 unit + 17 integration) with 81% coverage
7. **Deploy to production** ‚Äî Docker + nginx configuration with SSL, monitoring, and auto-scaling

---

## üèóÔ∏è What You'll Build

A complete full-stack web application featuring:

**Backend (FastAPI + Python)**:
- REST API with 3 endpoint groups (chat, providers, config)
- Provider abstraction layer supporting 6 LLM providers
- Session management with in-memory storage
- Debug metadata collection (14 performance metrics)
- Configuration presets and validation
- Comprehensive test suite (61 tests, 81% coverage)

**Frontend (React + TypeScript)**:
- Modern chat interface with message history
- 4 advanced components (Debug Panel, Config Panel, Export, Theme Toggle)
- Provider settings drawer with dynamic model selection
- Dark mode with localStorage persistence
- Full keyboard navigation and screen reader support
- Responsive design (mobile 320px, tablet 768px, desktop 1024px+)

**Infrastructure**:
- Docker Compose configuration for production
- Nginx reverse proxy with SSL termination
- Health checks and auto-restart policies
- Horizontal scaling (3+ backend replicas)
- Monitoring and logging integration

---

## ‚úÖ Implementation Status

### Phase 1: Basic Chat MVP ‚úÖ COMPLETE
- ‚úÖ Chat interface with message history
- ‚úÖ Session management (in-memory, UUID-based)
- ‚úÖ Mock provider for deterministic testing
- ‚úÖ REST API endpoints (/send, /sessions, /messages)
- ‚úÖ React frontend with TypeScript strict mode
- ‚úÖ 19 initial tests (API, models, services)

### Phase 2: Multi-Provider Support ‚úÖ COMPLETE
- ‚úÖ 6 LLM providers (Mock, Ollama, OpenAI, Anthropic, Google, Azure)
- ‚úÖ Dynamic model loading per provider
- ‚úÖ API key validation and session-based storage
- ‚úÖ Provider enumeration and factory pattern
- ‚úÖ Settings drawer UI component
- ‚úÖ 9 provider-specific tests

### Phase 3: Advanced Features ‚úÖ COMPLETE
- ‚úÖ Debug panel displaying 14 performance metrics
- ‚úÖ Configuration panel with 3 presets and sliders
- ‚úÖ Conversation export (JSON + Markdown formats)
- ‚úÖ Dark mode toggle with localStorage persistence
- ‚úÖ Keyboard shortcuts (Enter, Shift+Enter, Escape)
- ‚úÖ Responsive design (breakpoints: 320px, 768px, 1024px)
- ‚úÖ Full accessibility (ARIA labels, keyboard nav, semantic HTML)
- ‚úÖ 32 additional tests (debug, config, integration flows)
- ‚úÖ Production deployment documentation

**Total**: 61 tests, 81% code coverage, 13.36s execution time

---

## üìö Documentation (Learning Path)

**Recommended Order**: Start with QUICK_START ‚Üí Explore ARCHITECTURE ‚Üí Reference DEPLOYMENT for production

| Document | Purpose | Audience | Lines |
|----------|---------|----------|-------|
| **[QUICK_START.md](./QUICK_START.md)** | 5-minute setup guide with step-by-step instructions | Beginners, first-time users | 541 |
| **[ARCHITECTURE.md](./ARCHITECTURE.md)** | Technical design patterns, data flow, and component details | Intermediate engineers, architects | 590 |
| **[DEPLOYMENT.md](./DEPLOYMENT.md)** | Production deployment with Docker, nginx, SSL, monitoring | DevOps, SRE, production deployment | 500 |
| **[Backend README](./backend/README.md)** | API documentation, endpoints, and backend configuration | Backend developers | - |

---

## üéì Learning Modules (Theory ‚Üí Practice ‚Üí Reflection)

### Module 1: Basic Chat Interface (Phase 1)

**Theory**: REST API design, session management, provider abstraction patterns

**Hands-On**:
1. Run backend server: `cd web/backend && python main.py`
2. Explore Swagger UI: http://localhost:8000/docs
3. Test `/api/chat/send` endpoint with Mock provider
4. Examine session lifecycle in `chat_service.py`
5. Run unit tests: `pytest tests/test_api.py -v`

**Key Files to Study**:
- `backend/main.py` (116 lines) ‚Äî FastAPI app initialization, CORS, health check
- `backend/api/chat.py` (180 lines) ‚Äî Chat endpoints with validation
- `backend/services/chat_service.py` (110 lines) ‚Äî Business logic, session management
- `backend/models.py` (150 lines) ‚Äî Pydantic models with validation
- `frontend/src/components/Chat.tsx` (394 lines) ‚Äî Chat UI with state management

**Reflection Questions**:
- Why use UUIDs for session IDs instead of auto-incrementing integers?
- How does the provider abstraction enable multi-LLM support?
- What are the trade-offs of in-memory session storage vs database?

**Exercise**: Add a new endpoint `/api/chat/sessions/{session_id}/export` that returns conversation history as JSON

---

### Module 2: Multi-Provider Integration (Phase 2)

**Theory**: Factory pattern, strategy pattern, dynamic configuration, API key security

**Hands-On**:
1. Study provider abstraction in `provider_service.py`
2. Add API key for OpenAI: Set `OPENAI_API_KEY` environment variable
3. Switch providers mid-conversation (Mock ‚Üí OpenAI)
4. Compare model outputs for same prompt across providers
5. Run provider tests: `pytest tests/test_providers_api.py -v`

**Key Files to Study**:
- `backend/services/provider_service.py` (140 lines) ‚Äî Provider factory, 6 implementations
- `backend/api/providers.py` (70 lines) ‚Äî Provider enumeration, model loading
- `frontend/src/components/SettingsDrawer.tsx` (180 lines) ‚Äî Provider UI with validation
- `frontend/src/types/providers.ts` (40 lines) ‚Äî TypeScript type definitions

**Reflection Questions**:
- How does the factory pattern simplify adding new providers?
- Why store API keys in session memory instead of localStorage?
- What security considerations exist for API key transmission?

**Exercise**: Add support for a new provider (e.g., Cohere, Hugging Face, Mistral)

---

### Module 3: Debug & Observability (Phase 3)

**Theory**: Performance instrumentation, structured logging, debug metadata, cost attribution

**Hands-On**:
1. Enable debug mode: `POST /api/config/debug` with `{"enabled": true}`
2. Send message and inspect 14 debug metrics in response
3. Measure performance overhead (should be <50ms)
4. Export debug data via Debug Panel UI
5. Run debug tests: `pytest tests/test_debug_mode.py -v`

**Key Files to Study**:
- `backend/models.py` (lines 61-79) ‚Äî DebugMetadata model (14 fields)
- `backend/api/config.py` (120 lines) ‚Äî Debug toggle endpoint
- `frontend/src/components/DebugPanel.tsx` (97 lines) ‚Äî Debug metrics display

**14 Debug Metrics Captured**:
1. `input_tokens` ‚Äî Tokens in user prompt
2. `output_tokens` ‚Äî Tokens in assistant response
3. `total_tokens` ‚Äî Sum of input + output
4. `latency_ms` ‚Äî Total request latency
5. `provider` ‚Äî LLM provider used
6. `model` ‚Äî Model name
7. `agent_state` ‚Äî Orchestrator state
8. `tool_calls_count` ‚Äî Number of tool invocations
9. `reasoning_steps` ‚Äî Steps in reasoning chain
10. `error_details` ‚Äî Error messages if any
11. `session_config` ‚Äî Active configuration
12. `timestamp` ‚Äî ISO 8601 timestamp
13. `request_id` ‚Äî Unique request identifier
14. `cost_estimate` ‚Äî Estimated API cost (USD)

**Exercise**: Add 2 new debug metrics (cache_hit, retry_count)

---

### Module 4: Configuration Management (Phase 3)

**Theory**: Preset patterns, validation, user preferences, defaults

**Hands-On**:
1. Apply "Creative" preset: `GET /api/config/presets`
2. Customize parameters: max_turns=5, temperature=0.9
3. Compare outputs with "Precise" preset
4. Create custom preset with your preferred parameters
5. Run config tests: `pytest tests/test_config.py -v`

**3 Built-in Presets**:
| Preset | Temperature | Max Turns | Timeout | Use Case |
|--------|-------------|-----------|---------|----------|
| **Creative** | 0.9 | 5 | 60s | Brainstorming, storytelling |
| **Precise** | 0.3 | 3 | 30s | Factual Q&A, code generation |
| **Balanced** | 0.7 | 4 | 45s | General conversation |

**Exercise**: Create an "Experimental" preset (temp=1.5, turns=7, custom system prompt)

---

### Module 5: Testing & Quality (All Phases)

**Theory**: Test pyramid, mocking, coverage targets, CI/CD

**Test Suite (61 tests, 13.36s execution)**:
| Test File | Tests | Focus |
|-----------|-------|-------|
| `test_api.py` | 8 | Chat endpoints |
| `test_config.py` | 10 | Config validation |
| `test_models.py` | 7 | Model validation |
| `test_services.py` | 4 | Business logic |
| `test_providers_api.py` | 9 | Provider management |
| `test_debug_mode.py` | 10 | Debug accuracy |
| `test_integration_flows.py` | 13 | End-to-end workflows |

**Coverage**: 81% overall, 100% on models.py

**Exercise**: Write an integration test for provider switching

---

### Module 6: Production Deployment (DevOps)

**Theory**: Containerization, reverse proxies, SSL/TLS, horizontal scaling

**Hands-On**:
1. Build frontend: `npm run build`
2. Deploy with Docker Compose
3. Configure nginx reverse proxy
4. Add SSL certificate with Let's Encrypt
5. Monitor health checks and logs

**Production Architecture**:
```
Nginx (:80, :443) ‚Üí Backend Replicas (√ó3) ‚Üí LLM Providers
```

**Exercise**: Deploy to AWS/GCP/Azure with auto-scaling

---

## üöÄ Quick Start (5 Minutes)

### Prerequisites
- Python 3.11+ (backend)
- Node.js 16+ (frontend)
- Optional: Ollama for local LLM

### Setup

**Backend**:
```bash
cd web/backend
python -m venv venv
.\venv\Scripts\Activate.ps1  # Windows
pip install -r requirements.txt
python main.py  # http://localhost:8000
```

**Frontend**:
```bash
cd web/frontend
npm install
npm start  # http://localhost:3000
```

**Verify**: Open http://localhost:3000, send message with Mock provider

---

## üìÅ Project Structure

```
web/
‚îú‚îÄ‚îÄ README.md                       # This file (learning overview)
‚îú‚îÄ‚îÄ QUICK_START.md                  # 5-minute setup guide
‚îú‚îÄ‚îÄ ARCHITECTURE.md                 # Technical deep-dive
‚îú‚îÄ‚îÄ DEPLOYMENT.md                   # Production deployment
‚îÇ
‚îú‚îÄ‚îÄ frontend/                       # React 18 + TypeScript
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Chat.tsx            # Main chat (394 lines)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DebugPanel.tsx      # Debug metrics (97 lines)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ConfigPanel.tsx     # Config controls (232 lines)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ConversationExport.tsx  # Export (152 lines)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ThemeToggle.tsx     # Dark mode (31 lines)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SettingsDrawer.tsx  # Provider settings (180 lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chatService.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ providerService.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ configService.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ types/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ providers.ts
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ config.ts
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ
‚îî‚îÄ‚îÄ backend/                        # FastAPI Application
    ‚îú‚îÄ‚îÄ main.py                     # FastAPI init (116 lines)
    ‚îú‚îÄ‚îÄ models.py                   # Pydantic models (150 lines)
    ‚îú‚îÄ‚îÄ api/
    ‚îÇ   ‚îú‚îÄ‚îÄ chat.py                 # Chat endpoints (180 lines)
    ‚îÇ   ‚îú‚îÄ‚îÄ providers.py            # Provider endpoints (70 lines)
    ‚îÇ   ‚îî‚îÄ‚îÄ config.py               # Config endpoints (120 lines)
    ‚îú‚îÄ‚îÄ services/
    ‚îÇ   ‚îú‚îÄ‚îÄ chat_service.py         # Chat logic (110 lines)
    ‚îÇ   ‚îú‚îÄ‚îÄ provider_service.py     # Providers (140 lines)
    ‚îÇ   ‚îî‚îÄ‚îÄ config_service.py       # Config (95 lines)
    ‚îú‚îÄ‚îÄ tests/                      # 61 tests, 81% coverage
    ‚îÇ   ‚îú‚îÄ‚îÄ test_api.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_config.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_models.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_services.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_providers_api.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_debug_mode.py
    ‚îÇ   ‚îî‚îÄ‚îÄ test_integration_flows.py
    ‚îî‚îÄ‚îÄ requirements.txt
```

---

## üî¨ Key Concepts Demonstrated

1. **Provider Abstraction** ‚Äî Factory pattern for multi-LLM support
2. **Session Management** ‚Äî UUID-based, stateless HTTP with stateful conversations
3. **Debug Observability** ‚Äî 14-field metadata with optional collection
4. **Configuration Presets** ‚Äî Curated defaults with customization
5. **Test-Driven Development** ‚Äî 61 tests, 81% coverage, MockProvider
6. **Accessibility First** ‚Äî ARIA, keyboard nav, WCAG AA compliance

---

## üéì Learning Outcomes

After completing this lab, you will be able to:

‚úÖ Design REST APIs with FastAPI  
‚úÖ Implement provider abstraction for multiple LLMs  
‚úÖ Build accessible UIs with React + TypeScript  
‚úÖ Write comprehensive test suites  
‚úÖ Instrument applications with debug metadata  
‚úÖ Deploy to production with Docker + nginx  
‚úÖ Apply design patterns (Factory, Strategy, Observer)

---

## üîó Integration with Broader Curriculum

| Related Module | Connection |
|----------------|------------|
| **Labs 00-08** | Backend uses same `agent_labs` orchestrator |
| **Curriculum 02_intermediate** | Configuration patterns match curriculum |
| **Curriculum 03_advanced** | Debug observability aligns with production |
| **Projects P01-P12** | Can serve as frontend for projects |
| **Agents/ docs** | Architecture mirrors reference docs |

---

## üìä Success Metrics

| Metric | Target | Achieved |
|--------|--------|----------|
| Test Coverage | ‚â•80% | 81% ‚úÖ |
| Tests Passing | 100% | 61/61 ‚úÖ |
| Test Execution | <20s | 13.36s ‚úÖ |
| API Response | <500ms | ~200ms ‚úÖ |
| Debug Overhead | <50ms | <30ms ‚úÖ |
| Accessibility | WCAG AA | WCAG AA ‚úÖ |

---

## üõ†Ô∏è Tools & Technologies

**Backend**: Python 3.11, FastAPI 0.104, Pydantic v2, Uvicorn, pytest  
**Frontend**: React 18, TypeScript 5.9, CSS3  
**Infrastructure**: Docker, nginx, Let's Encrypt

---

## ü§ù Contributing

This is a learning resource. To contribute:
1. Study existing patterns
2. Add tests (maintain 80%+ coverage)
3. Update documentation
4. Follow space_framework governance

---

**Version**: 1.0.0 (Production Ready)  
**Last Updated**: 2026-01-26  
**Story**: #57 Web-Based Agent Chat Interface  
**Status**: ‚úÖ Complete (all 3 phases)
