# Environment Configuration for Interactive Agent Playground

# ============================================
# Ollama Configuration
# ============================================

# Base URL for Ollama service (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Model name to use with Ollama
# Options: mistral:7b, neural-chat, openchat, tinyllama, etc.
OLLAMA_MODEL=mistral:7b

# ============================================
# Agent Configuration
# ============================================

# Provider type: mock or ollama
AGENT_PROVIDER=ollama

# Maximum turns for agent to think/act
AGENT_MAX_TURNS=3

# ============================================
# Development Configuration
# ============================================

# Python logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Enable verbose output
VERBOSE=false

# ============================================
# Optional: LLM-Specific Configuration
# ============================================

# Temperature for LLM responses (0.0 to 2.0)
# Lower = more deterministic, Higher = more creative
LLM_TEMPERATURE=0.7

# Max tokens in response
LLM_MAX_TOKENS=500

# ============================================
# Optional: Memory Configuration
# ============================================

# Max short-term memory items
MEMORY_SHORT_TERM_MAX=10

# Enable long-term memory
MEMORY_LONG_TERM_ENABLED=true

# ============================================
# Optional: Tool Configuration
# ============================================

# Enable tool execution
TOOLS_ENABLED=true

# Tool timeout in seconds
TOOL_TIMEOUT=30
